nodes:
  unet_loader:
    class_type: UNETLoader
    title: "Load Diffusion Model"
    params:
      unet_name: "qwen_image_edit_fp8_e4m3fn.safetensors"
      weight_dtype: "default"
  clip_loader:
    class_type: CLIPLoader
    title: "Load Qwen CLIP"
    params:
      clip_name: "qwen_2.5_vl_7b_fp8_scaled.safetensors"
      type: "qwen_image"
      device: "default"
  vae_loader:
    class_type: VAELoader
    title: "Load VAE"
    params:
      vae_name: "qwen_image_vae.safetensors"
  lora_loader:
    class_type: LoraLoaderModelOnly
    title: "LoraLoaderModelOnly"
    params:
      lora_name: "Qwen-Image-Edit-Lightning-4steps-V1.0-bf16.safetensors"
      strength_model: 1.0
  
  pos_prompt_encoder:
    class_type: CLIPTextEncode
    title: "CLIP Text Encode (Positive)"
  neg_prompt_encoder:
    class_type: CLIPTextEncode
    title: "CLIP Text Encode (Negative)"
  
  latent_source:
    class_type: EmptySD3LatentImage
    title: "Empty Latent"
    params:
      batch_size: 1

  model_sampler:
    class_type: ModelSamplingAuraFlow
    params:
      shift: 3.0
  cfg_norm:
    class_type: CFGNorm
    params:
      strength: 1.0
  ksampler:
    class_type: KSampler
    title: "KSampler"
    params:
      steps: 4
      cfg: 1.0
      sampler_name: "euler"
      scheduler: "simple"
      denoise: 1.0

  vae_decode:
    class_type: VAEDecode
    title: "VAE Decode"
  save_image:
    class_type: SaveImage
    title: "Save Image"
    params: {}

connections:
  - from: "unet_loader:0"
    to: "lora_loader:model"
  - from: "lora_loader:0"
    to: "model_sampler:model"
  - from: "model_sampler:0"
    to: "cfg_norm:model"
  - from: "cfg_norm:0"
    to: "ksampler:model"
  - from: "clip_loader:0"
    to: "pos_prompt_encoder:clip"
  - from: "clip_loader:0"
    to: "neg_prompt_encoder:clip"

  - from: "pos_prompt_encoder:0"
    to: "ksampler:positive"
  - from: "neg_prompt_encoder:0"
    to: "ksampler:negative"
    
  - from: "vae_loader:0"
    to: "vae_decode:vae"
  - from: "latent_source:0"
    to: "ksampler:latent_image"
  - from: "ksampler:0"
    to: "vae_decode:samples"
  - from: "vae_decode:0"
    to: "save_image:images"

dynamic_reference_latent_chains:
  image_stitch_chain:
    vae_node: "vae_loader"
    output_map:
      positive: "pos_prompt_encoder:0"
      negative: "neg_prompt_encoder:0"
    end_input_map:
      positive: ["ksampler:positive"]
      negative: ["ksampler:negative"]

dynamic_lora_chains:
  lora_chain:
    template: "LoraLoader"
    output_map:
      "lora_loader:0": "model"
      "clip_loader:0": "clip"
    input_map:
      "model": "model"
      "clip": "clip"
    end_input_map:
      "model": ["model_sampler:model"]
      "clip": ["pos_prompt_encoder:clip", "neg_prompt_encoder:clip"]

ui_map:
  positive_prompt: "pos_prompt_encoder:text"
  negative_prompt: "neg_prompt_encoder:text"
  seed: "ksampler:seed"
  width: "latent_source:width"
  height: "latent_source:height"
  filename_prefix: "save_image:filename_prefix"