nodes:
  clip_loader:
    class_type: CLIPLoader
    title: "Load CLIP"
    params: { type: "wan", device: "default", clip_name: "umt5_xxl_fp8_e4m3fn_scaled.safetensors" }
  vae_loader:
    class_type: VAELoader
    title: "Load VAE"
    params: { vae_name: "wan_2.1_vae.safetensors" }
  unet_loader:
    class_type: UNETLoader
    title: "Load S2V UNET"
    params: { weight_dtype: "default", unet_name: "wan2.2_s2v_14B_fp8_scaled.safetensors" }
  lora_loader:
    class_type: LoraLoaderModelOnly
    title: "Load Lightning LoRA"
    params: { strength_model: 1.0, lora_name: "wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors" }
  audio_encoder_loader:
    class_type: AudioEncoderLoader
    title: "Load Audio Encoder"
    params: { audio_encoder_name: "wav2vec2_large_english_fp16.safetensors" }
  
  load_audio:
    class_type: LoadAudio
    title: "Load Audio"
  load_ref_image:
    class_type: LoadImage
    title: "Load Ref Image"
    
  pos_prompt:
    class_type: CLIPTextEncode
    title: "CLIP Text Encode (Positive)"
  neg_prompt:
    class_type: CLIPTextEncode
    title: "CLIP Text Encode (Negative)"
  audio_encoder:
    class_type: AudioEncoderEncode
    title: "Encode Audio"
  s2v_preprocessor:
    class_type: WanSoundImageToVideo
    title: "Prepare for S2V"
    params:
      batch_size: 1
      length: 77

  model_sampler:
    class_type: ModelSamplingSD3
    params: { shift: 8.0 }
  ksampler_first_chunk:
    class_type: KSampler
    title: "KSampler (First Chunk)"
    params:
      steps: 4
      cfg: 1.0
      sampler_name: "uni_pc"
      scheduler: "simple"
      denoise: 1.0
      
  vae_decode:
    class_type: VAEDecode
    title: "VAE Decode"
  image_from_batch:
    class_type: ImageFromBatch
    title: "Image From Batch"
    params:
      batch_index: 1
      length: 4096 
  create_video:
    class_type: CreateVideo
    params: { fps: 16 }
  save_video:
    class_type: SaveVideo
    params:
      format: "mp4"
      codec: "h264"

connections:
  - from: "unet_loader:0"
    to: "lora_loader:model"
  - from: "lora_loader:0"
    to: "model_sampler:model"
    
  - from: "clip_loader:0"
    to: "pos_prompt:clip"
  - from: "clip_loader:0"
    to: "neg_prompt:clip"
  - from: "load_audio:0"
    to: "audio_encoder:audio"
  - from: "audio_encoder_loader:0"
    to: "audio_encoder:audio_encoder"
  - from: "pos_prompt:0"
    to: "s2v_preprocessor:positive"
  - from: "neg_prompt:0"
    to: "s2v_preprocessor:negative"
  - from: "vae_loader:0"
    to: "s2v_preprocessor:vae"
  - from: "audio_encoder:0"
    to: "s2v_preprocessor:audio_encoder_output"
  - from: "load_ref_image:0"
    to: "s2v_preprocessor:ref_image"
  
  - from: "model_sampler:0"
    to: "ksampler_first_chunk:model"
  - from: "s2v_preprocessor:0"
    to: "ksampler_first_chunk:positive"
  - from: "s2v_preprocessor:1"
    to: "ksampler_first_chunk:negative"
  - from: "s2v_preprocessor:2"
    to: "ksampler_first_chunk:latent_image"
    
  - from: "ksampler_first_chunk:0"
    to: "vae_decode:samples"
  - from: "vae_loader:0"
    to: "vae_decode:vae"
  - from: "vae_decode:0"
    to: "image_from_batch:image"
  - from: "image_from_batch:0"
    to: "create_video:images"
  - from: "create_video:0"
    to: "save_video:video"
  - from: "load_audio:0"
    to: "create_video:audio"
    
dynamic_wan_s2v_chains:
  s2v_chain:
    start_node: "ksampler_first_chunk"
    end_node: "vae_decode"

ui_map:
  positive_prompt: "pos_prompt:text"
  negative_prompt: "neg_prompt:text"
  ref_image: "load_ref_image:image"
  audio_file: "load_audio:audio"
  aspect_ratio:
    width: "s2v_preprocessor:width"
    height: "s2v_preprocessor:height"
  seed: "ksampler_first_chunk:seed"
  filename_prefix: "save_video:filename_prefix"